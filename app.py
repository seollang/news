import streamlit as st
import requests
import aiohttp
import asyncio
from bs4 import BeautifulSoup
from transformers import pipeline
import re

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="IT News Summarizer", page_icon="ğŸ“°", layout="wide")

# ë‰´ìŠ¤ ë§í¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
@st.cache_data(ttl=3600)  # 1ì‹œê°„ ìºì‹±
def get_news_links():
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
    }
    try:
        response = requests.get("https://news.naver.com/section/105", headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")
        articles = soup.find_all("div", class_="sa_text")
        news_links = []
        seen_urls = set()
        for article in articles:
            link = article.find("a", href=re.compile(r'n\.news\.naver\.com/mnews/article/\d+/\d+'))
            title_tag = article.find("strong", class_="sa_text_strong")
            if link and title_tag:
                href = link.get("href")
                if href and href not in seen_urls:
                    if not href.startswith("http"):
                        href = "https://news.naver.com" + href
                    title = title_tag.get_text(strip=True) or "ì œëª© ì—†ìŒ"
                    news_links.append({"title": title, "url": href})
                    seen_urls.add(href)
        return news_links[:5]  # ìµœëŒ€ 5ê°œ ê¸°ì‚¬
    except Exception as e:
        st.error(f"ë‰´ìŠ¤ ë§í¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

# ë‰´ìŠ¤ ë³¸ë¬¸ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
@st.cache_data(ttl=3600)
async def get_article_content_async(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36"
    }
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=10) as response:
                response.raise_for_status()
                content = await response.text()
                soup = BeautifulSoup(content, "html.parser")
                article = soup.find("article", {"id": "dic_area"})
                return article.get_text(strip=True) if article else ""
    except Exception as e:
        st.error(f"ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return ""

# ìš”ì•½ í•¨ìˆ˜
@st.cache_resource
def get_summarizer():
    return pipeline("summarization", model="ainize/kobart-news")

@st.cache_data(ttl=3600)
def summarize_text(_text):
    try:
        summarizer = get_summarizer()
        max_input_length = 512
        summary = summarizer(_text[:max_input_length], max_length=150, min_length=30, do_sample=False)
        return summary[0]['summary_text']
    except Exception as e:
        st.error(f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return ""

# ë©”ì¸ ì•±
def main():
    st.title("ğŸ“° IT ë‰´ìŠ¤ ìš”ì•½ê¸°")
    st.markdown("ë„¤ì´ë²„ IT/ê³¼í•™ ë‰´ìŠ¤ë¥¼ ì„ íƒí•˜ì—¬ AIë¡œ ìš”ì•½í•©ë‹ˆë‹¤.")

    # ë‰´ìŠ¤ ë§í¬ ê°€ì ¸ì˜¤ê¸°
    with st.spinner("ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
        news_links = get_news_links()

    if not news_links:
        st.warning("ê°€ì ¸ì˜¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        return

    # ë‰´ìŠ¤ ì„ íƒ ë“œë¡­ë‹¤ìš´
    selected_news = st.selectbox(
        "ìš”ì•½í•  ë‰´ìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš”:",
        options=news_links,
        format_func=lambda x: x['title']
    )

    if selected_news:
        st.write(f"[ì›ë¬¸ ì½ê¸°]({selected_news['url']})")
        with st.spinner("ê¸°ì‚¬ë¥¼ ìš”ì•½í•˜ëŠ” ì¤‘..."):
            # ë¹„ë™ê¸°ì ìœ¼ë¡œ ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            article = loop.run_until_complete(get_article_content_async(selected_news['url']))
            if article:
                summary = summarize_text(article)
                if summary:
                    st.markdown("### ìš”ì•½")
                    st.write(summary)
                else:
                    st.warning("ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.warning("ê¸°ì‚¬ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
